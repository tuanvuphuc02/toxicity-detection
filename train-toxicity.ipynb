{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import re\n",
    "import os, sys\n",
    "import datetime, time\n",
    "import gc, operator \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import pkg_resources\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from config import SEED, PARTIAL_TRAIN, TEST_SIZE, NUM_LABELS\n",
    "from config import MAX_SEQUENCE_LENGTH, NUM_EPOCH, LEARNING_RATE, BATCH_SIZE\n",
    "from config import ACCUMULATION_STEPS, INPUT_DIR, WORK_DIR, TOXICITY_COLUMN, DATA_DIR\n",
    "from config import BERT_MODEL_NAME, FINE_TUNED_MODEL_PATH\n",
    "\n",
    "from utils import set_seed, convert_lines_onfly, preprocess\n",
    "from utils import calculate_overall_auc, compute_bias_metrics_for_model, get_final_metric\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "\n",
    "\n",
    "device = torch.device('cuda')\n",
    "TOXICITY_COLUMN = \"target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 902437 records\n",
      "CPU times: user 26.4 s, sys: 416 ms, total: 26.8 s\n",
      "Wall time: 26.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
    "train_df = train_df.sample(frac=PARTIAL_TRAIN, random_state=SEED)\n",
    "print('loaded %d records' % len(train_df))\n",
    "\n",
    "test_df = train_df.tail(TEST_SIZE)\n",
    "train_df = train_df.head(((train_df.shape[0]-TEST_SIZE)//BATCH_SIZE)*BATCH_SIZE)\n",
    "\n",
    "# Make sure all comment_text values are strings\n",
    "sentences = preprocess(train_df['comment_text'].astype(str).fillna(\"DUMMY_VALUE\")).values \n",
    "train_df = train_df.fillna(0)\n",
    "\n",
    "# List all identities\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "y_columns = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat'] + identity_columns\n",
    "identity_sp = [ 'homosexual_gay_or_lesbian','muslim', 'black', 'white']\n",
    "\n",
    "# Convert taget and identity columns to booleans\n",
    "train_df = train_df.drop(['comment_text'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELS = len(y_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weights(train):\n",
    "    has_identity = torch.sigmoid(10*(torch.tensor((train[identity_columns].fillna(0).max(axis=1)).values)-0.4))\n",
    "    has_target = torch.sigmoid(10*(torch.tensor(train['target'].values)-0.4))\n",
    "    weights = (torch.ones(train.shape[0],dtype=torch.float64)+has_identity+has_identity*(1-has_target)+has_target*(1-has_identity)) / 4\n",
    "    weights = weights.to(dtype=torch.float32)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure every batch has similar sentences length, and shuffle the batchs\n",
    "sort_idx = np.argsort(np.array([len(x.split()) for x in sentences])).reshape(train_df.shape[0]//BATCH_SIZE,BATCH_SIZE)\n",
    "np.random.shuffle(sort_idx)\n",
    "sort_idx = sort_idx.reshape(train_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 129 ms, sys: 12.1 ms, total: 141 ms\n",
      "Wall time: 140 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sentences = sentences[sort_idx]\n",
    "X = sentences                #[train_df.idx]\n",
    "y = train_df[y_columns].values[sort_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_tensor = calculate_weights(train_df)[sort_idx].repeat(len(y_columns),1).transpose(0,1)\n",
    "weights_tensor[:,0] = weights_tensor[:,0] * (len(y_columns))/4\n",
    "weights_tensor[:,6:] = weights_tensor[:,6:] * 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.utils.data.TensorDataset(torch.arange(len(X)), torch.tensor((np.abs(2.0*y-1.0)**0.5*np.sign(y-0.5)+1)/2,dtype=torch.float), weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(BERT_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "\n",
    "model.zero_grad()\n",
    "_ = model.cuda()\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "avg_val_loss = 0.\n",
    "avg_val_accuracy = 0.\n",
    "num_train_optimization_steps = int(NUM_EPOCH*len(train)/BATCH_SIZE/ACCUMULATION_STEPS)\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47947750154e4167a9e55fdb0d170c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fdb76019e7a4c17af43f53194e86fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = model.train()\n",
    "tq = tqdm(range(NUM_EPOCH))\n",
    "for epoch in tq:\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    avg_loss = 0.\n",
    "    avg_accuracy = 0.\n",
    "    lossf = None\n",
    "    optimizer.zero_grad()\n",
    "    tk0 = tqdm(train_loader,leave = False)\n",
    "    for i , (ind_batch, y_batch, w_batch) in enumerate(tk0):\n",
    "        ind_batch.requires_grad = False\n",
    "        x_batch=torch.tensor(convert_lines_onfly(X[ind_batch.numpy()], MAX_SEQUENCE_LENGTH, tokenizer))\n",
    "        y_pred = model(x_batch.to(device)).logits\n",
    "        loss =  F.binary_cross_entropy_with_logits(y_pred, y_batch.to(device), weight=w_batch.to(device)) / ACCUMULATION_STEPS\n",
    "\n",
    "        loss.backward()\n",
    "        if (i+1) % ACCUMULATION_STEPS == 0:             # Wait for several backward steps\n",
    "            optimizer.step()                            # Now we can do an optimizer step\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if lossf:\n",
    "            lossf = 0.98*lossf+0.02*loss.item()*ACCUMULATION_STEPS\n",
    "        else:\n",
    "            lossf = loss.item()\n",
    "        tk0.set_postfix(loss = lossf)\n",
    "        avg_loss += loss.item()*ACCUMULATION_STEPS / len(train_loader)\n",
    "        avg_accuracy += torch.mean(((torch.sigmoid(y_pred[:,0])>0.5) == (y_batch[:,0]>0.5).to(device)).to(torch.float) ).item()/len(train_loader)\n",
    "    tq.set_postfix(avg_val_loss=avg_val_loss,avg_val_accuracy=avg_val_accuracy,avg_loss=avg_loss,avg_accuracy=avg_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), FINE_TUNED_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subgroup</th>\n",
       "      <th>subgroup_size</th>\n",
       "      <th>subgroup_auc</th>\n",
       "      <th>bpsn_auc</th>\n",
       "      <th>bnsp_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>homosexual_gay_or_lesbian</td>\n",
       "      <td>595</td>\n",
       "      <td>0.852852</td>\n",
       "      <td>0.902126</td>\n",
       "      <td>0.969667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>black</td>\n",
       "      <td>715</td>\n",
       "      <td>0.882556</td>\n",
       "      <td>0.911155</td>\n",
       "      <td>0.972392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>white</td>\n",
       "      <td>1269</td>\n",
       "      <td>0.894304</td>\n",
       "      <td>0.913037</td>\n",
       "      <td>0.975115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>muslim</td>\n",
       "      <td>1085</td>\n",
       "      <td>0.905086</td>\n",
       "      <td>0.938026</td>\n",
       "      <td>0.968383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jewish</td>\n",
       "      <td>379</td>\n",
       "      <td>0.911916</td>\n",
       "      <td>0.948676</td>\n",
       "      <td>0.963780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>psychiatric_or_mental_illness</td>\n",
       "      <td>223</td>\n",
       "      <td>0.936339</td>\n",
       "      <td>0.948994</td>\n",
       "      <td>0.972985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>2727</td>\n",
       "      <td>0.942418</td>\n",
       "      <td>0.970372</td>\n",
       "      <td>0.961011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>christian</td>\n",
       "      <td>1962</td>\n",
       "      <td>0.957535</td>\n",
       "      <td>0.974280</td>\n",
       "      <td>0.961560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>2104</td>\n",
       "      <td>0.957539</td>\n",
       "      <td>0.964077</td>\n",
       "      <td>0.972543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        subgroup  subgroup_size  subgroup_auc  bpsn_auc  \\\n",
       "2      homosexual_gay_or_lesbian            595      0.852852  0.902126   \n",
       "6                          black            715      0.882556  0.911155   \n",
       "7                          white           1269      0.894304  0.913037   \n",
       "5                         muslim           1085      0.905086  0.938026   \n",
       "4                         jewish            379      0.911916  0.948676   \n",
       "8  psychiatric_or_mental_illness            223      0.936339  0.948994   \n",
       "1                         female           2727      0.942418  0.970372   \n",
       "3                      christian           1962      0.957535  0.974280   \n",
       "0                           male           2104      0.957539  0.964077   \n",
       "\n",
       "   bnsp_auc  \n",
       "2  0.969667  \n",
       "6  0.972392  \n",
       "7  0.975115  \n",
       "5  0.968383  \n",
       "4  0.963780  \n",
       "8  0.972985  \n",
       "1  0.961011  \n",
       "3  0.961560  \n",
       "0  0.972543  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.9492865378389379"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run validation\n",
    "# The following 2 lines are not needed but show how to download the model for prediction\n",
    "sentences = preprocess(test_df['comment_text'].astype(str).fillna(\"DUMMY_VALUE\")).values\n",
    "test_df=test_df.fillna(0)\n",
    "sort_idx=np.flip(np.argsort(np.array([len(x.split()) for x in sentences])))\n",
    "org_idx=np.argsort(sort_idx)\n",
    "X = sentences[sort_idx]\n",
    "test_preds = torch.zeros((len(X)))\n",
    "x_test = torch.arange(len(X))\n",
    "test = torch.utils.data.TensorDataset(x_test)\n",
    "model = BertForSequenceClassification.from_pretrained(BERT_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "model.load_state_dict(torch.load(FINE_TUNED_MODEL_PATH))\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "_ = model.cuda()\n",
    "_ = model.eval()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "tk0 = tqdm(test_loader,leave=False)\n",
    "tranct = 0\n",
    "for i, (ind_batch,) in enumerate(tk0):\n",
    "    x_batch=torch.tensor(convert_lines_onfly(X[ind_batch.numpy()], MAX_SEQUENCE_LENGTH, tokenizer))\n",
    "    y_pred = model(x_batch.to(device)).logits\n",
    "    test_preds[i * BATCH_SIZE:(i+1) * BATCH_SIZE] = test_preds[i * BATCH_SIZE:(i+1) * BATCH_SIZE]+torch.sigmoid(y_pred[:, 0].cpu())\n",
    "    tranct = tranct + BATCH_SIZE * (x_batch.shape[1] == MAX_SEQUENCE_LENGTH)\n",
    "    tk0.set_postfix(trunct=tranct,gpu_memory=torch.cuda.memory_allocated() // 1024 ** 2,batch_len=x_batch.shape[1])\n",
    "    \n",
    "MODEL_NAME = 'model1'\n",
    "test_df[MODEL_NAME]=torch.sigmoid(torch.tensor(test_preds[org_idx])).numpy()\n",
    "TOXICITY_COLUMN = 'target'\n",
    "bias_metrics_df = compute_bias_metrics_for_model(test_df, identity_columns, MODEL_NAME, 'target')\n",
    "bias_metrics_df\n",
    "get_final_metric(bias_metrics_df, calculate_overall_auc(test_df, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
